{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Implementation of a RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the OpenAI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='2 + 2 is equal to 4.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 15, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-cae16d7c-197f-4812-a9b7-730ca4e539ee-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",  # or gpt-4 if available\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Test the model with a simple query:\n",
    "print(model.invoke(\"How much is 2+2?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Transcribe the YouTube Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think it's possible that physics has exploits and we should be trying to find them. arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow gives you\n"
     ]
    }
   ],
   "source": [
    "# Load the transcript (if already transcribed)\n",
    "with open(\"transcription.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "# Optionally, print a snippet to verify the content\n",
    "print(transcript[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Transcript into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 270\n",
      "Sample chunk: I think it's possible that physics has exploits and we should be trying to find them. arranging some kind of a crazy quantum mechanical system that so\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define chunk size and overlap (adjust these parameters as needed)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_text(transcript)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(\"Sample chunk:\", chunks[0][:150])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings and Build a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create an embeddings object\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Build an in-memory vector store from the text chunks\n",
    "vectorstore = FAISS.from_texts(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Relevant Context for a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context:\n",
      " And then they can, they can just, they contribute noise and entropy into everything. And they blow stuff. And also organizationally has been really fascinating to me that it can be very distracting. If you, if all, if you only want to get to work as vision, all the resources are on it and you're building out a data engine. And you're actually making forward progress because that is the, the sensor with the most bandwidth, the most constraints on the world. And you're investing fully into that and you can make that extremely good. If you're, you're only a finite amount of sort of spend of focus across different facets of the system. And this kind of reminds me of reach sudden is a bit of lesson. It just seems like simplifying the system. Yeah. In the long run, not, of course, you know, know what the long way is. It seems to be always the right solution. Yeah. In that case, it was for RL, but it seems to apply generally across all systems that do computation. So where, what do you think\n",
      "comes to the very large extent to what we are trying to achieve in the product format, what we're trying to, the release we're trying to get out in the feedback from the Q18 worth it where the system is struggling or not, the things we're trying to improve. And the Q18 gives some signal, some information in aggregate about the performance of the system in various conditions. And then of course all of us drive it and we can also see it. It's really nice to work with a system that you can also experience yourself and it drives you home. Is there some insight you can draw from your individual experience that you just can't quite get from an aggregate statistical analysis of data? Yeah, it's so weird, right? Yes. It's not scientific in a sense because you're just one anecdotal sample. Yeah, I think there's a ton of, it's a source of truth. It's your interaction with the system and you can see it, you can play with it, you can perturb it, you can get a sense of it, you have an intuition\n",
      "us because no one has built AGI again. So all we have available to us is, is there enough for CalGround on the periphery? I would say yes. And we have the progress so far, which has been very rapid. And there are next steps that are available. And so I would say, yeah, it's quite likely that will be interacting with digital entities. How do you know that we, somebody's ability to, it's going to be a slow, I think it's going to be a slow incremental transition, it's going to be product based and focused. It's going to be GitHub Copial getting better. And then GPD's helping you write. And then these oracles that you can go to with mathematical problems. I think we're on a, on a verge of being able to ask very complex questions in chemistry, physics, math of these oracles and have them complete solutions. So AGI to use primarily focused on intelligence. So consciousness doesn't enter into it. So in my mind, consciousness is not a special thing. You will, you will figure out and bolt on,\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the benefits of RAG applications?\"\n",
    "retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "# Combine the retrieved document chunks into one context string\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "print(\"Retrieved context:\\n\", context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prompt Template and Build the Final Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_16140\\4084215163.py:13: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=model, prompt=prompt)\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_16140\\4084215163.py:16: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(context=context, question=query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response: I don't know\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define a prompt template\n",
    "template = \"\"\"Answer the question based on the context below.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "If you cannot answer, reply \"I don't know\".\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the chain by connecting the prompt with the model\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "# Run the chain with the current query and retrieved context\n",
    "response = chain.run(context=context, question=query)\n",
    "print(\"Final response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pinecone Instead of FAISS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'youtube-transcripts' has dimension 1024 but expected 1536.\n",
      "Deleting the existing index and re-creating it with the correct dimension...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.vectorstores import Pinecone as LC_Pinecone\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys and settings from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")  # 'youtube-transcripts'\n",
    "\n",
    "# Expected dimension for your embeddings\n",
    "EXPECTED_DIMENSION = 1536\n",
    "\n",
    "# Create a Pinecone client instance\n",
    "pc = Pinecone(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV\n",
    ")\n",
    "\n",
    "# Check if the index exists\n",
    "existing_indexes = [index.name for index in pc.list_indexes()]\n",
    "if INDEX_NAME in existing_indexes:\n",
    "    # Get details of the existing index\n",
    "    index_info = pc.describe_index(INDEX_NAME)\n",
    "    if index_info.dimension != EXPECTED_DIMENSION:\n",
    "        print(f\"Index '{INDEX_NAME}' has dimension {index_info.dimension} but expected {EXPECTED_DIMENSION}.\")\n",
    "        print(\"Deleting the existing index and re-creating it with the correct dimension...\")\n",
    "        pc.delete_index(INDEX_NAME)\n",
    "        # Wait for deletion to complete\n",
    "        time.sleep(10)\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=EXPECTED_DIMENSION,\n",
    "            metric=\"euclidean\",\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",        # Adjust based on your cloud provider\n",
    "                region=PINECONE_API_ENV  # Uses 'us-east-1' from your .env file\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Index '{INDEX_NAME}' already exists with the correct dimension.\")\n",
    "else:\n",
    "    # Create the index since it doesn't exist\n",
    "    print(f\"Index '{INDEX_NAME}' does not exist. Creating a new one with dimension {EXPECTED_DIMENSION}...\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=EXPECTED_DIMENSION,\n",
    "        metric=\"euclidean\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",        # Adjust based on your cloud provider\n",
    "            region=PINECONE_API_ENV  # Uses 'us-east-1' from your .env file\n",
    "        )\n",
    "    )\n",
    "\n",
    "# (Make sure that 'chunks' and 'embeddings' are defined before calling this.)\n",
    "vectorstore = LC_Pinecone.from_texts(chunks, embeddings, index_name=INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing missing packages: ['faiss-cpu']\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List the required packages\n",
    "required_packages = [\n",
    "    \"python-dotenv\",\n",
    "    \"pytube\",\n",
    "    \"openai-whisper\",\n",
    "    \"langchain\",\n",
    "    \"openai\",\n",
    "    \"faiss-cpu\"\n",
    "]\n",
    "\n",
    "# Get a set of installed package names (all lower-case)\n",
    "installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "\n",
    "# Find out which packages are missing\n",
    "missing = [pkg for pkg in required_packages if pkg.lower() not in installed_packages]\n",
    "\n",
    "if missing:\n",
    "    print(\"Installing missing packages:\", missing)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing)\n",
    "else:\n",
    "    print(\"All required packages are already installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pinecone' object has no attribute 'get_index_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\Jupyter projects\\Projects\\RAG-Applications\\Youtube Transcript RAG\\rag.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Projects/RAG-Applications/Youtube%20Transcript%20RAG/rag.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Get the index-specific client for your index\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Projects/RAG-Applications/Youtube%20Transcript%20RAG/rag.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m index_client \u001b[39m=\u001b[39m pc\u001b[39m.\u001b[39;49mget_index_client(index_name\u001b[39m=\u001b[39mINDEX_NAME)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Projects/RAG-Applications/Youtube%20Transcript%20RAG/rag.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Retrieve and print the index statistics\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Jupyter%20projects/Projects/RAG-Applications/Youtube%20Transcript%20RAG/rag.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m stats \u001b[39m=\u001b[39m index_client\u001b[39m.\u001b[39mdescribe_index_stats()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pinecone' object has no attribute 'get_index_client'"
     ]
    }
   ],
   "source": [
    "# Get the index-specific client for your index\n",
    "index_client = pc.get_index_client(index_name=INDEX_NAME)\n",
    "\n",
    "# Retrieve and print the index statistics\n",
    "stats = index_client.describe_index_stats()\n",
    "print(\"Index Stats:\", stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU resources initialized. Faiss GPU installation is working!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "try:\n",
    "    # Try initializing GPU resources.\n",
    "    res = faiss.StandardGpuResources()  \n",
    "    print(\"GPU resources initialized. Faiss GPU installation is working!\")\n",
    "except Exception as e:\n",
    "    print(\"GPU resources could not be initialized:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
